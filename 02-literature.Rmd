# Literature

## 자료준비

국내 주요 언론에서 코로나19 상황에 대한 위험과 기회를 어느 정도로 실재적인 측면을 반영하는지 가늠하기 위해, 코로나19 상황의 위험과 기회를 반영하는 주제어를 구성하고, 이 주제어를 통해 반지도학습 토픽모델링을 수행한다. 

데이터는 `tidyverse`와 `quanteda`를 이용해 마련하고, LDA토픽모델링은 `seededlda`를 이용한다. 

`quanteda`패키지는 `dfm`에 메타정보를 추가하는데 사용한다. `quanteda`에서는  한글형태소 분석기를 사용할 수 없기 때문에, `tidyverse`와 `tidytext`로 먼저 형태소분석한 데이터프레임을 만들어 `quanteda`에 투입한다. 


.

#### 패키지 로딩
```{r mc1, message=FALSE, warning=FALSE}
pkg_v <- c(
  "tidyverse", "tidytext", "lubridate", "quanteda", "readtext", "seededlda")
purrr::map(pkg_v, require, ch = T)
```
.

### 기사 자료 확보

말뭉치는 빅카인즈를 통해 확보한다. 코로나19 관련 기사를 추출하기 위해 다음의 방법을통해 기사를 검색해 다운로드 받는다. 빅카인즈는 저작권 문제로 기사는 200자만 제공하지만, 기사 전문에 대해 키워드를 제공한다. 이 키워드는 형태소 분석을 통한 추출한 명사에 해당한다. 

1. 키워드
((코로나19) OR (코로나) OR (코로나 바이러스) OR (신종 코로나바이러스) OR (COVID-19) OR (코비드19))

2. 세부 설정
- 언론사: 
경향 국민 동아 문화 서울 세계 조선 중앙 한겨레 한국 KBS MBC SBS 

- 분류
사회: 의료건강
인사, 부고, 동정 등 제외

- 기간: 2021년 5월 1일부터 10월 31일 
2021-05-01 ~ 2021-10-31 
16,288건
(분석편의를 위해 2만건을 넘기지 않도록 했다. 빅카인즈는 한번에 2만건 까지만 다운로드를 받을 수 있도록 했다. 2만건이 넘어가면 일반 개인 컴퓨터로는 분석시간이 오래 걸리는 문제도 있다.)

.

다운로드 받은 파일을 작업디렉토리의 `data`폴더에 복사한다. 제대로 복사돼 있는지 확인. 

```{r mc1-1}
list.files(path = 'data', pattern = '^News.*\\.xlsx$')
```
.

5월 ~ 10월에 해당하는 파일 선택. 

```{r mc1-2}
file_path <- "data/NewsResult_20210501-20211031.xlsx"
readxl::read_excel(file_path) %>%
  glimpse()
```
.

분석에 필요한 열만 선택한다. 

```{r mc1-3}
readxl::read_excel(file_path) %>%
  select(일자, 제목, 본문, 키워드, 언론사, cat = `통합 분류1`,  URL) -> vac_df
vac_df %>% head(3)
```

.

정제. 불용어 부호 중복기사 공백 등 제거

```{r mc1-4}
fullchar_v <- "ㆍ|ㅣ|‘|’|“|”|○|●|◎|◇|◆|□|■|△|▲|▽|▼|〓|◁|◀|▷|▶|♤|♠|♡|♥|♧|♣|⊙|◈|▣"

vac_df %>%
  # 인터뷰 기고 등 제거
  filter(!str_detect(제목, "(
    \\[인터뷰|\\인터뷰\\]|
    \\[전문|전문\\]|
    \\[기고|기고\\]|
    \\[Q|A\\]|
    \\[논담|논담\\]|
    \\[좌담회|좌담회\\]|
    답\\>|
    답\\]|
    \\<파워|
    \\[사람|사람\\]|
    \\[탐방|탐방\\]|
    \\[속보|속보\\]|
    \\[팩트|
    \\[브리핑|브리핑\\]|
    \\[시평|시평\\]|
    )")) %>% 
  # 중복기사 제거
  distinct(제목, .keep_all = T) %>%
  # 기사 공백제거
  mutate(제목 = str_squish(제목)) %>%
  # 기사 공백제거
  mutate(본문 = str_squish(본문)) %>%
  # 특수문자 제거
  mutate(키워드 = str_remove_all(키워드, "[^(\\w+|\\d+|,)]")) %>%
  mutate(키워드 = str_remove_all(키워드, fullchar_v)) %>%
  # 기사별 ID부여
  mutate(ID = factor(row_number())) %>%
  # 월별로 구분한 열 추가(lubridate 패키지) %>% 
  mutate(ym = str_sub(일자, 1, 6)) %>% 
  mutate(ym = as.integer(ym)) %>% 
  mutate(title = 제목) %>% 
  # 기사 제목과 본문 결합
  unite(제목, 본문, col = "text", sep = " ") %>% 
  #키워드 갯수 계산
  mutate(Nword = str_count(키워드, pattern = ',')) %>%
  relocate(Nword, after = 일자) %>%
  # 기사 분류 구분
  separate(cat, sep = ">", into = c("cat", "cat2")) %>%
  # IT_과학, 경제, 사회 만 선택
  select(-cat2) %>%
  # 분류 구분: 사회, 비사회
  relocate(cat, after = Nword) %>%
  mutate(catSoc = case_when(
    cat == "사회" ~ "사회면",
    cat == "지역" ~ "사회면",
    TRUE ~ "비사회면") ) -> vac2_df

vac2_df %>% glimpse()
```
.

정제 데이터 확인

```{r mc1-4a}
vac2_df$ym %>% unique()

```
.

사회 분류의 의료_건강만 선택했지만, 다른 영역의 기사도 포함된다. 

```{r mc1-4b}
vac2_df %>% count(cat, sort = T)
```
.

기사 길이 확인 

```{r mc1-4c}
vac2_df %>% .$Nword %>% summary()
```
.

기사의 길이가 일정 수준까지는 기사의 품질과 정비례의 관계가 있다는 전제하에 단어수가 70개 이상한 기사만 선택.

```{r mc1-4d}
vac2_df %>%
  filter(Nword >= 70) -> vac2_df
vac2_df %>% .$Nword %>% summary()
```
.

tidytext 방식으로 토큰화. 

```{r mc1-5a}
vac2_df %>%
  unnest_tokens(word, 키워드, token = "regex", pattern = ",") -> vac_tk
vac_tk %>% 
  select(ym, title, word) %>% 
  head(n = 5)
```
.

토큰 빈도 계산

```{r mc1-5b}
vac_tk %>% 
  count(word, sort = T) -> count_df 
count_df %>% head(n = 10)
```
.

tidytext방식(한 행에 하나의 값 배치)의 토큰을 각 기사의 행에 재배치 `text2`열에 할당한다. 다른 변수가 포함된 데이터프레임과 결합.  `quanteda`로 `dfm`을 만들기 위해 필요한 작업. `dfm`에 개별 기사에 대한 변수가 포함돼 있어야 추가적인 분석이 가능하다. 

```{r mc1-5c}
combined_df <-
  vac_tk %>%
  group_by(ID) %>%
  summarise(text2 = str_flatten(word, " ")) %>%
  ungroup() %>%
  inner_join(vac2_df, by = "ID")

combined_df %>% glimpse()
combined_df %>% saveRDS("combined.rds")
```

.
.
.

