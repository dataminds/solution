# 분석2: 상대빈도 {#anal2comp }

## 개관  


```{r cmp1, message=FALSE, warning=FALSE}
pkg_v <- c("tidyverse", "tidytext","RcppMeCab")
lapply(pkg_v, require, ch = T)

```

단어(term) - 문서(document) - 말뭉치(corpus)

문서에 사용된 단어(term)를 통해 그 문서의 주제를 추론할 수 있다. 말뭉치는 문서들의 '뭉치'다. 신문 말뭉치는 여러 신문들의 기사를 모아놓은 것이고, 소설말뭉치는 여러 소설을 모아놓은 문서들의 집합이다. 말뭉치는 상대적인 개념이다. 예를 들어, 소설집을 말뭉치라고 하면 개별 소설이 문서가 된다. 반면, 소설 한편을 말뭉치라고 하면, 소설의 각 장이 문서가 된다. 

개별 문서의 주제를 추론하려면 모든 문서에 걸쳐 사용빈도가 높은 단어보다는 개별 문서에서만 사용빈도가 높은 단어를 찾아야 한다. 즉, 개별 문서에 등장하는 단어의 상대빈도가 높은 단어가 개별 문서의 의미를 잘 나타낸다. 

상대빈도를 계산하는 방법으로 널리 사용되는 지표가 tf_idf, 승산비, 가중로그승산비 등이다. 

- tf_idf

널리 사용되는 상대빈도 지표. 문서 전반에 걸쳐 등장하는 단어의 점수를 낮게 계산하고, 특정 문서에서 등장하는 빈도에 점수를 높게 부여해 상대 빈도를 계산한다. 예를 들어, '운수좋은 날'과 '사랑손님과 어머니' 등 2편으로 이뤄진 소설집 말뭉치가 있다고 하자. "은/는/이/가"와 같은 단어는 이 말뭉치에가 사용빈도가 꽤 높지만, 각 소설의 주제를 파악하는데 크게 기여하지 못한다. 반면 '어머니'와 같은 단어는 '운수좋은 날'에는 등장하지 않고, '사랑손님과 어머니'에는 많이 등장해, 해당 문서의 주제를 파악하는데 크게 기여한다. 

- 승산비(odds ratio)

승산(odds)의 비를 이용해 복수의 문서에 등장한 단어의 상대적인 빈도를 계산한다. 2종의 문서에 대해서만 사용할 수 있다. 

- 가중 로그 승산비(weighted log odds)

베이지언 확률모형으로 가중치를  계산해 단어의 상대적인 빈도를 계산한다. tf_idf와 승산비의 단점을 보완한 방법이다. 



## tf_idf

단어빈도-역문서빈도(term frequency-inverse document frequency: tf_idf).

말 그대로 개별 문서의 단어빈도(tf)와 문서전반에 걸쳐 사용된 정도의 역수(idf)를 곱해 구한다. 




### 공식

tf_idf의 요소인 tf와 idf에 대해 각각 알아보자. 

##### tf(term frequency)

개별 문서(d)에 등장하는 단어(t)의 수를 각 문서에 등장한 모든 단어의 수로 나눈 값. 

$$
tf(t, d) = \frac{tf_{document}}{tf_{total}}
$$

- $tf_{document}$: 각 문서에 등장한 해당 단어의 빈도
- $tf_{total}$: 각 문서에 등장한 모든 단어의 수


##### df(document frequency)

특정 단어(t)가 나타난 문서(D)의 수. df가 높다면 '은/는/이/가'처럼 대부분의 문서에서 사용되는 단어임을 나타낸다. 

$$
df(t, D)
$$


##### idf(inverse document frequency)

전체 문서의 수(N)를 해당 단어의 df로 나눈 뒤 로그를 취한 값. 값이 클수록 특정 문서에서만 등장하는 특이한 단어라는 의미가 된다. 

$$
idf(t, D) = log(\frac{N}{df})
$$


##### tf_idf
tf와 idf를 곱하면 tf_idf를 구할 수 있다. 


$$
  tfidf(t, d, D) = tf(t, d) \times idf(t, D)
$$

- `t`: 단어
- `d`: 개별 문서
- `D`: 개별 문서의 집합(말뭉치)

### 적용 

짧은 문장을 이용해 tf_idf의 원리를 파악해 보자. 먼저 토큰화한다. 여기서 각 행은 개별 문서에 해당하고, 4개 행의 합의 말뭉치에 해당한다. 각 행을 식별하기 위해 문장별로 토큰화해 ID를 부여하자. 

```{r cmp2 }
sky_v <-  c( "The sky is blue.", 
           "The sun is bright today.",
           "The sun in the sky is bright.", 
           "We can see the shining sun, the bright sun.")

sky_doc <- tibble(text = sky_v) %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  mutate(lineID = row_number()) %>% 
  mutate(lineID = as.factor(lineID))

sky_doc

```

#### tf

개별 문서(d)에 등장하는 단어(t)의 수를 각 문서에 등장한 모든 단어의 수로 나눈 값. 

$$
tf(t, d) = \frac{tf_{document}}{tf_{total}}
$$

- $tf_{document}$: 각 문서에 등장한 해당 단어의 빈도
- $tf_{total}$: 각 문서에 등장한 모든 단어의 수

tf의 분자인 $tf_{document}$(각 문서에 등장한 해당 단어의 빈도)를 구한다. 여기서 각 문서는 각 행이다. 따라서 각 행에서 등장한 단어의 빈도를 계산하면 된다.   

```{r cmp3 }
sky_tfd <- 
sky_doc %>% 
  unnest_tokens(word, sentence) %>% 
  anti_join(stop_words) %>% 
  count(lineID, word) %>% 
  rename(tfdoc = n)
  
sky_tfd

```

tf의 분모 $tf_{total}$(각 문서에 등장한 모든 단어의 수)를 구한다. 각 행을 이루는 모든 단어의 수를 계산하면 된다. 

```{r cmp4 }
sky_tft <- sky_tfd %>% 
  mutate(N = length(unique(lineID))) %>%  #total number of documnets
  count(lineID) %>% 
  rename(tftotal = n)

sky_tft

```

$tf_{document}$(각 문서에 등장한 해당 단어의 빈도)를 $tf_{total}$(각 문서에 등장한 모든 단어의 수)로 나눈다. 

```{r cmp5 }
sky_tf <- left_join(sky_tfd, sky_tft) %>% 
  mutate(tf = tfdoc/tftotal)

sky_tf

```




#### df(document frequency)

특정 단어(t)가 나타난 문서(D)의 수. df가 높다면 '은/는/이/가'처럼 대부분의 문서에서 사용되는 단어임을 나타낸다. 

$$
df(t, D)
$$
특정 단어가 등장한 문서의 수를 계산해 구할 수 있다. 
  
```{r cmp6 }
sky_df <- table(sky_tf$word) %>% 
  as.data.frame() %>% 
  rename(word = Var1, df = Freq)
sky_df

```

'blue': 1번행에만 등장했으므로 1

'sun': 2,3, 4번 행에 등장했으므로 3
  
  
#### idf(inverse document frequency)

전체 문서의 수(N)를 해당 단어의 df로 나눈 뒤 로그를 취한 값. 값이 클수록 특정 문서에서만 등장하는 특이한 단어라는 의미가 된다. 

$$
idf(t, D) = log(\frac{N}{df})
$$

공식을 그대로 적용해 계산한다. `sky_tf`에 N의 값을 계산해 두 데이터프레임을 열방향 결합한다. 

```{r cmp7 }
sky_tf <- 
  sky_tf %>% 
  mutate(N = length(unique(lineID))) #total number of documnets

sky_idf <- 
  left_join(sky_tf, sky_df) %>% 
  mutate(idf = log(N / df))

sky_idf

```

3개 행에 걸쳐 등장하는 'bright'의 idf가 1개 행에만 등장한 'blue'보다 idf가 작다. 


#### tf_idf
tf와 idf를 곱하면 tf_idf를 구할 수 있다. 


$$
  tfidf(t, d, D) = tf(t, d) \times idf(t, D)
$$

- `t`: 단어
- `d`: 개별 문서
- `D`: 개별 문서의 집합(말뭉치)


```{r cmp8 }
sky_idf %>% 
  mutate(tf_idf = tf * idf) %>% 
  arrange(-tf_idf)


```




### `bind_tf_idf(tbl, term, document, n)`

`tidytext`패키지에서 제공하는 `bind_tf_idf()`함수를 이용하면 tf_idf를 계산할 수 있다. 

- tbl: 정돈데이터(한행에 값 하나).

- term: 문자열이나 기호 등의 단어가 저장된 열.

- document: 문자열이나 기호 등의 문서식별부호가 저장된 열.

- n: 문자열이나 기호 등의 문서-용어의 빈도가 저장된 열.	


```{r cmp9 }
sky_tfd %>% 
  bind_tf_idf(tbl = ., term = word, document = lineID, n = tfdoc)

```




## 승산비(odds ratio)

문서의 상대빈도를 구하는 또 다른 방법이 승산비다. 승산(odds)의 비를 이용해 복수의 문서에 등장한 단어의 상대적인 빈도 계산할 수 있다. 따라서 승산비는 2종의 문서에 대해서만 구할 수 있다. 

- 승산비(odds ratio)

한 사건의 승산(odds)에 대한 다른 한 사건 승산의 비(ratio)다. 다음은 승산B에 대한 승산A의 비. 

$$
승산비 = \frac{승산A}{승산B}
$$


- 비(ratio)

두 부분 중 한 부분에 대한 다른 한 부분의 비. 예를 들어, 축구경기를 할때 한국이 3골을 넣고, 일본이 1골을 넣었다면, 비는 3대 1로서, 일본팀 1점에 대한 한국팀 3점의 비(ratio)다. 

$$
A:B = \frac{A}{B}
$$

- 승산(odds)

어느 한 사건이 일어날 가능성이 승산(odds)이다. 영어를 그대로 읽어 '오즈'라고도 한다. 한 사건의 발생빈도(n)를 전체의 값(total)으로 나눈다. 

$$
승산(odds) = \frac{발생빈도(n) + 1}{총빈도(total) + 1}
$$
분자와 분모에 각각 1을 더하는 이유는 발생빈도가 0인 경우도 있기 때문이다. 분모가 0이면 무한대가 된다. 



### 자료준비

네이버영화 댓글의 승산비를 구해보자. 댓글을 긍정과 부정으로 구분해 라벨링한 자료를 이용한다.

- [Naver sentiment movie corpus](https://github.com/e9t/nsmc)


```{r cmp10 , eval=FALSE}
url_v <- 'https://github.com/e9t/nsmc/raw/master/ratings.txt'
dest_v <- 'data/ratings.txt'

download.file(url_v, dest_v, mode = "wb")
list.files('data/.')

```

다운로드한 자료 이입 및 검토

```{r cmp11 }
read_lines("data/ratings.txt") %>% glimpse()

```

공백문자 `\t`으로 구분된 문자데이터이므로, `read_tsv()`로 이입한다. 

```{r cmp12 }
read_tsv("data/ratings.txt") %>% glimpse()

```

```{r cmp13 }
ratings_df <- read_tsv("data/ratings.txt") 
head(ratings_df)

```

데이터셋이 커 처리에 시간이 걸리므로 각 라벨별로 1000개 행씩 추출한다. 

```{r cmp14 }
set.seed(37)
by1000_df <- 
  ratings_df %>% 
  group_by(label) %>% 
  sample_n(size = 1000)

by1000_df %>% glimpse()

```

`RcppMeCab`패키지에서 `pos()`함수로 형태소를 추출한 뒤, 일반명사('nng')만 추출해 빈도를 계산한다. 

```{r cmp15 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word, sort = T)

```




#### 자료 준비

영화평은 긍정적인 평에는 '1', 부정적인 평에는 '0'으로 분류돼 있다. 긍정적인 평과 부정적인 평에 사용된 명사의 승산비를 구해보자. 이를 위해 long form을 wide form으로 변형해 `label`열의 값을 열의 헤더로 변환하고, 각 열의 값으로는 토큰을 투입한다. 결측값이 있으면 연산이 안되므로 `NA`값은 '0'으로 채운다. 


```{r cmp }
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0))

```


#### 승산비 계산

긍정평과 부정평의 승산을 구한다음, 승산비를 계산한다.

```{r cmp16 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0)) %>% 
  rename(posi = `1`, nega = `0`) %>% 
  mutate(odds_posi = ((posi+1)/sum(posi+1)),
         odds_nega = ((nega+1)/sum(nega+1))) %>% 
  mutate(posi_odds_ratio = (odds_posi / odds_nega)) 
  

```


승산비를 이용해 영화의 긍정평과 부정평에서 상대적으로 많이 사용된 명사를 추출했다. 순서대로 정렬하자. 


```{r cmp17 }
rate_odds_df <- 
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0)) %>% 
  rename(posi = `1`, nega = `0`) %>% 
  mutate(odds_posi = ((posi+1)/sum(posi+1)),
         odds_nega = ((nega+1)/sum(nega+1))) %>% 
  mutate(posi_odds_ratio = (odds_posi / odds_nega)) %>% 
  # 긍정평과 부정평 각각 상위 20개씩 필터 
  filter(rank(posi_odds_ratio) <= 20 | rank(-posi_odds_ratio) <= 20) %>%   arrange(-posi_odds_ratio)

rate_odds_df %>% head()
rate_odds_df %>% tail()
  
```


'밋'과 'ㄷ'이 사용된 문장을 살펴보자. `drop = `인자에 `FALSE`를 투입하면 토큰화한 문장을 제거하지 않고 남겨둔다. 

```{r cmp18 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word == "밋" | word == "ㄷ") 

```

'재밋었음'에서 `MeCab`형태소 분석기가 '밋'을 명사로 추출한 결과이다. 

```{r cmp19 }
enc2utf8("재밋었음10자는뭐여") %>% pos()

```


막대도표로 시각화하자. 긍정평과 부정평 도표를 분리해 표시하기 위해 승산비 1을 기준으로 크면 '긍정평' 작으면 '부정평'을 부여한다. 

```{r cmp20 }
rate_odds_df %>% 
  mutate(label = ifelse(posi_odds_ratio > 1, "긍정평", "부정평")) %>% 
  mutate(word = reorder(word, posi_odds_ratio)) %>% 
  
  ggplot(aes(x = posi_odds_ratio, 
             y = word, 
             fill = label)) + 
  geom_col(show.legend = F) +
  facet_wrap(~label, scales = "free")

```


중요도가 비슷한 단어를 추출해보자. 승산비가 1이면 분자와 분모가 같으므로 긍정평과 부정평의 승산이 같다는 의미다. 

```{r cmp21 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0)) %>% 
  rename(posi = `1`, nega = `0`) %>% 
  mutate(odds_posi = ((posi+1)/sum(posi+1)),
         odds_nega = ((nega+1)/sum(nega+1))) %>% 
  mutate(posi_odds_ratio = (odds_posi / odds_nega)) %>% 
  # 승산비 1을 중심으로 정렬
  arrange(abs(1 - posi_odds_ratio)) %>% 
  head(20)

```

중요도가 비슷하면서 빈도가 높은 단어를 찾아 보자. 

```{r cmp22 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0)) %>% 
  rename(posi = `1`, nega = `0`) %>% 
  # 긍정평과 부정평에서 각각 10회 초과한 단어 filter
  filter(nega > 10 & posi > 10) %>%  
  mutate(odds_posi = ((posi+1)/sum(posi+1)),
         odds_nega = ((nega+1)/sum(nega+1))) %>% 
  mutate(posi_odds_ratio = (odds_posi / odds_nega)) %>% 
  arrange(abs(1 - posi_odds_ratio)) %>% 
  head(20)

```



### 로그승산비

- 로그승산비(log odds ratio)
승산비에 로그를 위한 값. 로그를 취하면 1보다 작은수는 음수가 되고, 1보다 큰수는 양수가 된다. 

$$
로그 승산비 = log(\frac{승산A}{승산B})
$$


```{r cmp23 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0)) %>% 
  rename(posi = `1`, nega = `0`) %>% 
  mutate(odds_posi = ((posi+1)/sum(posi+1)),
         odds_nega = ((nega+1)/sum(nega+1))) %>% 
  # 승산비에 로그를 취한다.
  mutate(log_odds_ratio = log(odds_posi / odds_nega)) 
  

```



로그 승산비를 이용하면 하나의 도표에 상대빈도를 표시할 수 있다. 먼저 로그승산비를 구한뒤, 0을 기준으로 긍정평과 부정평을 group으로 구분한다. 긍정평과 부정평 집단별로 구분돼 있으므로, 로그승산비의 절대값 상위 10개를 지정하면, 긍정평과 부정평 별로 각각 상위 10개 단어를 추출할 수 있다. 


```{r cmp24 }
rate_log_df <- 
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  pivot_wider(names_from = label,
              values_from = n, 
              values_fill = list(n = 0)) %>% 
  rename(posi = `1`, nega = `0`) %>% 
  mutate(odds_posi = ((posi+1)/sum(posi+1)),
         odds_nega = ((nega+1)/sum(nega+1))) %>% 
  # 승산비에 로그를 취한다.
  mutate(log_odds_ratio = log(odds_posi / odds_nega)) 

rate_log_df  %>% 
  group_by(label  = ifelse(log_odds_ratio > 0, "긍정평", "부정평")) %>% 
  # 긍정평과 부정평별 각각 상위 10개 
  slice_max(abs(log_odds_ratio), n = 10) 
  


```

막대도표로 시각화.  

```{r cmp25 }
rate_log_df %>% 
  group_by(label  = ifelse(log_odds_ratio > 0, "긍정평", "부정평")) %>% 
  slice_max(abs(log_odds_ratio), n = 10) %>% 
  
  ggplot(aes(x = log_odds_ratio,
             y = reorder(word, log_odds_ratio),
             fill = label)) +
  geom_col() 


```









## 가중로그승산비

로그승산비는 문서 2종에 대한 승산으로 비를 구하므로, 3종 이상의 문서로 구성된 말뭉치에 적용할 수 없는 한계가 있다. tf_idf는 3종 이상의 문서로 구성된 말뭉치에 적용할 수 있지만, tf_idf계산 방식에서 오는 한계가 있다. 모든 문서에 걸쳐 등장하는 단어라 해서 반드시 불용어처럼 문서의 의미파악에 기여하지 못하는 것이 아니기 때문이다. 

tf_idf의 한계를 Tyler Schnoebelen이 [Monroe, Colaresi, and Quinn(2008)](Monroe, Colaresi, and Quinn(2008)의 연구를 토대 블로그문서
["I dare say you will never use tf-idf again"](https://medium.com/@TSchnoebelen/i-dare-say-you-will-never-use-tf-idf-again-4918408b2310)을 통해 지적하며, 대안으로 로그승산비에 베이지언 확률모형을 적용한 [가중로그승산비(weighted log odds ratio)](https://bookdown.org/Maxine/tidy-text-mining/weighted-log-odds-ratio.html)를 제안했다. 

이에 `tidytext` 개발자인 Julia Silge가 [`tydylo`](https://github.com/juliasilge/tidylo)패키지를 개발해 가중로그승산비를 간단하게 계산할 수 있도록 했다. ([Silge의 패키지 소개 글](https://juliasilge.com/blog/introducing-tidylo/))


```{r cmp26 , eval=FALSE}
install.packages("tidylo")

```


### `bind_log_odds(tbl, set, feature, n, uninformative = FALSE, unweighted = FALSE)`


- tbl: 정돈데이터(feature와 set이 하나의 행에 저장).

- set: feature를 비교하기 위한 set(group)에 대한 정보(예: 긍정 vs. 부정)이 저장된 열.

- feature: feature(단어나 바이그램 등의 텍스트자료)가 저장된 열.

- n: feature-set의 빈도를 저장한 열.

- uninformative: uninformative 디리슐레 분포 사용 여부. 기본값은 `FALSE`.	

- unweighted: 비가중 로그승산 사용여부. 기본값은 `FALSE`. `TRUE`로 지정하면 비가중 로그승산비(`log_odds`) 열을 추가한다. 

가중로그승산비를 이용해 네이버영화 댓글 중 긍정평과 부정평에 사용된 단어의 상대빈도를 구해보자. 

```{r cmp27 }
library(tidylo)

weighted_log_odds_df <- 
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  bind_log_odds(set = label,
                feature = word,
                n = n) %>% 
  arrange(-log_odds_weighted)

weighted_log_odds_df

```

긍정평과 부정평별 가중로그승산비 상위 10개 추출한다.


```{r cmp28 }
weighted_log_odds_df %>%   
  group_by(label = ifelse(label > 0, "긍정평", "부정평")) %>% 
  slice_max(abs(log_odds_weighted), n = 10) # 긍정평과 부정평별 각각 상위 10개 


```

막대도표로 시각화한다.

```{r cmp29 }
weighted_log_odds_df %>%   
  group_by(label = ifelse(label > 0, "긍정평", "부정평")) %>% 
  slice_max(abs(log_odds_weighted), n = 10) %>%  # 긍정평과 부정평별 각각 상위 10개 
  
  ggplot(aes(x = log_odds_weighted,
             y = reorder(word, log_odds_weighted),
             fill = label)) +
  geom_col(show.legend = F) +
  facet_wrap(~label, scale = "free")


```


부정평으로 분류된 내용 중 '최고'와 '감동'이 부정평에 들어 있다. 어떤 문장인지 확인해보자. 


```{r cmp30 }
by1000_df %>% 
  unnest_tokens(word, document, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  filter(label == 0) %>% 
  filter(word == "최고" | word == "감동")


```

'억지로 끼워 맞추려는 감동 설정' '감동 없음' 등 단어 하나만을 토큰으로 사용했을 때의 한계를 잘 보여주는 사례. 

'감동적 그자체 영화'를 부정평으로 분류한 것은 라벨링 오류. 



### 지표 비교

#### 로그승산비


먼저 가중치를 사용하지않은 로그승산비를 구한다.  


```{r cmp31 }
rate_log_df  %>% # 승산비에 로그를 취한다.
  group_by(label  = ifelse(log_odds_ratio > 0, "긍정평", "부정평")) %>% 
  slice_max(abs(log_odds_ratio), n = 10) %>%  # 긍정평과 부정평별 각각 상위 10개 
  
  ggplot(aes(x = log_odds_ratio,
             y = reorder(word, log_odds_ratio),
             fill = label)) +
  geom_col(show.legend = F) +
  facet_wrap(~label, scale = "free")

```



#### tf_idf

tf_idf 계산

```{r cmp32 }
tf_idf_df <- 
by1000_df %>% 
  unnest_tokens(word, document, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == "nng") %>% 
  filter(word != "영화") %>% 
  count(word) %>% 
  bind_tf_idf(term = word,
              document = label,
              n = n)
tf_idf_df

```

긍정평과 부정평별 상위 10개 추출한다.

```{r cmp33 }
tf_idf_df %>% 
  group_by(label = ifelse(label > 0, "긍정평", "부정평")) %>% 
  slice_max(tf_idf, n = 10) # 긍정평과 부정평별 각각 상위 10개 

```

막대도표로 시각화한다.

```{r cmp34 }
tf_idf_df %>% 
  group_by(label = ifelse(label > 0, "긍정평", "부정평")) %>% 
  slice_max(tf_idf, n = 10) %>% # 긍정평과 부정평별 각각 상위 10개 
  
  ggplot(aes(x = tf_idf,
             y = reorder(word, tf_idf),
             fill = label)) +
  geom_col(show.legend = F) +
  facet_wrap(~label, scale = "free")


```



#### 비교 

가중로그승산비, 로그승산비, tf_idf의 값을 비교해보자. 먼저 3개 데이터프레임을 행방향으로 결합해 하나의 데이터프레임으로 저장한다. 


```{r cmp35 }
wlo_df <- 
weighted_log_odds_df %>%   
  group_by(label = ifelse(label > 0, "긍정평", "부정평")) %>% 
  slice_max(abs(log_odds_weighted), n = 10) %>% 
  select(label, word, score = log_odds_weighted)

rlo_df <- 
rate_log_df  %>% 
  group_by(label  = ifelse(log_odds_ratio > 0, "긍정평", "부정평")) %>% 
  slice_max(abs(log_odds_ratio), n = 10) %>% 
  select(label, word, score = log_odds_ratio) 

ti_df <- 
tf_idf_df %>% 
  group_by(label = ifelse(label > 0, "긍정평", "부정평")) %>% 
  slice_max(tf_idf, n = 10) %>% 
  select(label, word, score = tf_idf) %>% 
  mutate(score = score * 600)

bind_rows(wlo_df, rlo_df, ti_df, .id = "ID") %>% tail(20)




```



```{r cmp36 }
bind_rows(wlo_df, rlo_df, ti_df, .id = "ID") %>% 
  mutate(ID = ifelse(ID == "1", "1.가중로그승산비", 
                     ifelse(ID == "2", "2.로그승산비", "3.tf_idf"))
         ) %>% 
  
  ggplot(aes(x = score,
             y = reorder(word, score),
             fill = label)
         ) +
  geom_col(show.legend = F) +
  facet_wrap( ~ label+ID, scales = "free", ncol = 3)
```




## 연습

'백신'관련 보도의 내용을 빅카인즈의 자료를 이용해 탐색해보자. 


### 빅카인즈

[빅카인즈](https://www.bigkinds.or.kr/)는 언론진흥재단이 운영하는 뉴스빅데이터 분석서비스다. 종합일간지, 경제지, 지역일간지, 방송사 등의 기사를 분석할수 인터페이스를 제공한다. 빅카인즈인터페이스를 통해 
빅카인즈에서 제공하는 인터페이스으로 뉴스텍스트를 분석할 수 있다. 관계도, 키워드트렌드, 연관어분석 등이 가능하다. 사용자가 직접 분석할 수 있도록 데이터다운로드 서비스도 제공한다. 무료데이터는 일자, 언론사, 기고자, 제목, 분류, 인물, 위치, 본문(200자), 기사ULR 등이 포함돼 있다. 

- 데이터다운로도 방법: 

1. 빅카인즈 접솝
2. 상단 메뉴의 '뉴스분석' 클릭
3. '뉴스분석' 메뉴가 아래로 펼쳐지면 '뉴스·검색 분석' 클릭. 
4. 'STEP01·뉴스검색' 창에서 다운로드할 기사의 범위 설정.
5. 'STEP03·분석결과 및 시각화'를 클릭하면 '데이터다운로드' 메뉴가 나온다. 
6. 오른쪽 하단의 '엑셀다운로드' 클릭. 


#### 백신관련 보도 수집

모든 기사를 분석하기 위해서는 시간이 많이 걸리므로, 데이터셋의 크기를 줄이기 위해, 분석 대상을 2021년 2월 한달간 '경향신문' '한겨레' '문화일보' '조선일보' 등 4개 일간지에서 보도된 내용에 국한하자. 

`4. 'STEP01·뉴스검색'` 창에서 다운로드할 기사의 범위 설정

 - 검색어: '백신'
 - 기간: 2021년 2월 1일 ~ 2월 28일
 - 통합분류: '사회', 'IT_과학'
 - 언론사: '경향신문' '한겨레' '문화일보' '조선일보' 선택

작업디렉토리 아래 `data`폴더에 다운로드 받는다. 

파일명을 확인한다. `NewsResult_20210201-20210228.xlsx`다. 

```{r cmp37 , eval=FALSE}
list.files("data/.")

```

`readxl`패키지의 `read_excel()`함수로 해당 파일을 R환경으로 이입한다. 


```{r cmp38 ,eval=FALSE}
readxl::read_excel("data/NewsResult_20210201-20210228.xlsx") %>% glimpse()

```

분석에 활용할 열만 추출해 `vac_df`에 할당한다.

```{r cmp39 }
vac_df <- 
  readxl::read_excel("data/NewsResult_20210201-20210228.xlsx") %>% 
  select(제목, 언론사, 본문, URL)

vac_df %>% glimpse()

```


### 정제 

먼저 `drop_na()`함수로 결측값을 제거하고, 토큰화한 다음, `pos`함수로 일반명사만 추출해, `vac_tk`로 저장한다. 


```{r cmp40 }
vac_tk <- 
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng')

vac_tk

```


### 분석 및 소통 

총빈도, 감정어빈도, tf_idf, 로그승산비, 가중로그승산비 등 다양한 분석 방법을 배웠다. 총빈도와 가중로그승산비를 구해 언론사별 어떤 차이가 있는지 살펴보자. (분석 결과를 해석할 때는 2021년 2월 한달 기간에 국한한 자료를 이용한 점에 주의해야 한다.) 


#### 언론사별 총빈도

```{r cmp41 }
vac_tk %>% 
  count(언론사, word, sort = T) %>% 
  filter(word != "경향") %>% 
  filter(word != "포토") %>% 
  filter(word != "문화") %>% 
  filter(word != "조선") %>% 
  filter(word != "한겨레") %>% 
  filter(word != "백신") %>%
  filter(word != "접종") %>% 
  group_by(언론사) %>% 
  slice_max(n, n = 7) %>% 
  
  ggplot(aes(x = n,
             y = reorder(word, n),
             fill = 언론사)
         ) +
  geom_col(show.legend = F) +
  facet_wrap( ~ 언론사, scales = "free")

```


주요 단어가 사용된 내용(제목)을 살펴보자.

```{r cmp42 }
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng') %>% 
  filter(word == "카") %>% .$제목

```

'카'의 빈도가 높았던 이유는 '아스트라제네카'를 형태소분석기가 하나의 일반명사로 추출하지 않았기 때문이다. 

```{r cmp43 }
enc2utf8("아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단") %>% pos()


```


```{r cmp44 }
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng') %>% 
  filter(word == "이상") %>% .$제목

```


#### 언론사별 가중로그승산비


```{r cmp45 }
vac_tk %>% 
  count(언론사, word) %>% 
  filter(word != "경향") %>% 
  filter(word != "포토") %>% 
  filter(word != "문화") %>% 
  filter(word != "조선") %>% 
  filter(word != "한겨레") %>%
  filter(word != "백신") %>%
  filter(word != "접종") %>% 
  bind_log_odds(set = 언론사,
                feature = word,
                n = n) %>% 
  group_by(언론사) %>% 
  slice_max(abs(log_odds_weighted), n = 7) %>% 
  
  ggplot(aes(x = log_odds_weighted,
             y = reorder(word, log_odds_weighted),
             fill = 언론사)
         ) +
  geom_col(show.legend = F) +
  facet_wrap( ~ 언론사, scales = "free")


```

주요 단어가 사용된 내용(제목)을 살펴보자.

```{r cmp46 }
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng') %>% 
  filter(언론사 == "경향신문") %>% 
  filter(word == "현장") %>% .$제목

```


```{r cmp47 }
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng') %>% 
  filter(언론사 == "문화일보") %>% 
  filter(word == "확정") %>% .$제목

```


```{r cmp48 }
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng') %>% 
  filter(언론사 == "조선일보") %>% 
  filter(word == "국민") %>% .$제목

```


```{r cmp49 }
vac_df %>% 
  drop_na() %>% 
  unnest_tokens(word, 제목, token = pos, drop = F) %>% 
  separate(word, c("word", "pos"), sep = "/") %>% 
  filter(pos == 'nng') %>% 
  filter(언론사 == "한겨레") %>% 
  filter(word == "뒤") %>% .$제목

```


### 총빈도와 상대빈도 비교

언론사별로 각 기사에서 사용된 단어의 총빈도와 상대빈도를 비교하자. 

먼저 총빈도를 구한다. 

```{r cmp50 }
total_fq <- 
vac_tk %>% 
  count(언론사, word, sort = T) %>% 
  filter(word != "경향") %>% 
  filter(word != "포토") %>% 
  filter(word != "문화") %>% 
  filter(word != "조선") %>% 
  filter(word != "한겨레") %>% 
  filter(word != "백신") %>%
  filter(word != "접종") %>% 
  group_by(언론사) %>% 
  slice_max(n, n = 7) 

total_fq

```

상대빈도

```{r cmp51 }
wlo_fq <- 
vac_tk %>% 
  count(언론사, word) %>% 
  filter(word != "경향") %>% 
  filter(word != "포토") %>% 
  filter(word != "문화") %>% 
  filter(word != "조선") %>% 
  filter(word != "한겨레") %>%
  filter(word != "백신") %>%
  filter(word != "접종") %>% 
  bind_log_odds(set = 언론사,
                feature = word,
                n = n) %>% 
  group_by(언론사) %>% 
  slice_max(abs(log_odds_weighted), n = 7)

wlo_fq 

```

총빈도와 상대빈도 데이터프레임을 행방향으로 결합한다. 

```{r cmp52 }
bind_rows(
  select(total_fq, 언론사, word, score = n),
  select(wlo_fq, 언론사, word, score = log_odds_weighted),
  .id = "ID") %>% 
  mutate(ID = ifelse(ID == 1, "total", "wlo")) %>% 
  
  ggplot(aes(x = score,
             y = reorder(word, score),
             fill = 언론사)
         ) +
  geom_col(show.legend = F) +
  facet_wrap(~ID+언론사, scales = "free", ncol = 4)
  


```

전반적으로 4개 언론사는 백신과 관련하여 2021년 2월 한달간 '이상'(특정 연령 이상)과 아스트라제네카 백신에 대한 내용을 주로 다뤘다. 

언론사별로는 경향신문은 '오늘'과 '현장', 문화일보는 '미국'과 '누적', 조선일보는 '국민'과 '속도', 한겨레는 '뒤'와 '팔' 등에 관련된 기사를 다른 언론사에 비해 상대적으로 더 다뤘다. 




#### 과제

빅카인즈에서 언론사별로 관심 분야의 기사를 추출해 보도 내용을 분석할 수 있도록 시각화하시오. 

- 보도에 사용된 단어의 총빈도에 대해 선택한 언론사별 비교 시각화

- 보도에 사용된 단어의 상대빈도에 대해 선택한 언론사별 비교 시각화

- 총빈도와 상대빈도에 대해 언론사별로 비교 시각화 







